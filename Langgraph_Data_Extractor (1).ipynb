{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "y0W3dHLxYQkZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlsFcOTCCmly"
      },
      "outputs": [],
      "source": [
        "!pip install langgraph langchain_core langchain_groq langchain_huggingface langchain_chroma"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict,Literal,Annotated,List\n",
        "from langgraph.graph import StateGraph,START,END\n",
        "from langchain_groq import ChatGroq\n",
        "import os\n",
        "from google.colab import userdata,drive\n",
        "drive.mount('/content/drive')\n",
        "os.environ['GROQ_API_KEY']=userdata.get('GROQ_API_KEY')\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import  ToolNode\n",
        "from pydantic import  BaseModel,Field\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from google.colab import drive\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.retrievers import MultiQueryRetriever\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough,RunnableParallel\n",
        "from langchain.tools import tool\n",
        "from typing_extensions import TypedDict\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVArB2n7D1QX",
        "outputId": "0888bacd-e908-473c-8594-c29509092c63"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a34ONsDHHsWM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AVh832QbLw1Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Extractor and Formatter**"
      ],
      "metadata": {
        "id": "y0W3dHLxYQkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Graphstate(TypedDict):\n",
        "    user_name:str\n",
        "    topic:str\n",
        "    input_query:str\n",
        "    due_date:str\n",
        "    validation:Literal['valid','invalid']\n",
        "    messages:str\n",
        "\n",
        "class LLMOutput(BaseModel):\n",
        "    user_name:str=Field(description='EmailId or Username')\n",
        "    topic:str=Field(description='Topic or main content of user query')\n",
        "    due_date:str=Field(description=('Date mentioned in user query'))\n",
        "\n",
        "class Dataverify(BaseModel):\n",
        "  output:str\n",
        "\n",
        "def data_extractor(state:Graphstate):\n",
        "    llm=ChatGroq(model='llama-3.1-8b-instant').with_structured_output(LLMOutput)\n",
        "\n",
        "    prompt=f\"\"\"Kindly extract below mentioned data from user input message{state['input_query']}\n",
        "              1)User name or User Email id\n",
        "              2)Topic\n",
        "              3)Date(MM/DD/YYYY)\n",
        "              Provide your output in json format\"\"\"\n",
        "\n",
        "    response=llm.invoke(prompt)\n",
        "\n",
        "    return {'user_name':response.user_name,\n",
        "            'topic':response.topic,\n",
        "            'due_date':response.due_date,\n",
        "            'messages':'Our query is being processed. We have prepared your data'}\n",
        "\n",
        "\n",
        "def store_data(state:Graphstate):\n",
        "  import pandas as pd\n",
        "  received_info=pd.DataFrame(columns=['Username','Date','Topic'])\n",
        "  received_info.loc[0,'Username']=state.get('user_name')\n",
        "  received_info.loc[0,'Date']=state.get('due_date')\n",
        "  received_info.loc[0,'Topic']=state.get('topic')\n",
        "  return {'messages':f'Your necessary informations are stored {received_info}. Mail has sent to your mailbox '}\n",
        "\n",
        "def query_verifier(state:Graphstate):\n",
        "  llm=ChatGroq(model='llama-3.1-8b-instant').with_structured_output(Dataverify)\n",
        "  prompt=f\"\"\"Check whether below informations are available in {state['input_query']}\n",
        "             1)User Data\n",
        "             2)Email ID\n",
        "             Give your answer in JSON format with a key 'output' and a value of 'yes' or 'no'\n",
        "             \"\"\"\n",
        "\n",
        "  validation=llm.invoke(prompt)\n",
        "\n",
        "  return {'validation':validation.output,\n",
        "          'messages':'Necessary informations are available'}\n",
        "\n",
        "\n",
        "def user_redirecting(state:Graphstate):\n",
        "  return {'messages':'Email ID or Date not available. Kindly provide it'}\n",
        "\n",
        "\n",
        "def router(state:Graphstate):\n",
        "\n",
        "  result=state.get('validation')\n",
        "  if result.lower()=='yes':\n",
        "    return 'valid'\n",
        "  else:\n",
        "    return 'invalid'\n",
        "\n",
        "\n",
        "graph=StateGraph(Graphstate)\n",
        "\n",
        "graph.add_node('query_verify',query_verifier)\n",
        "graph.add_node('input_redirecting',user_redirecting)\n",
        "graph.add_node('data_extractor',data_extractor)\n",
        "graph.add_node('sending_data',store_data)\n",
        "\n",
        "graph.set_entry_point('query_verify')\n",
        "graph.add_conditional_edges('query_verify',\n",
        "                            router,\n",
        "                            {'valid':'data_extractor',\n",
        "                             'invalid':'input_redirecting'}\n",
        "                            )\n",
        "graph.add_edge('data_extractor','sending_data')\n",
        "graph.add_edge('sending_data',END)\n",
        "graph.add_edge('input_redirecting',END)\n",
        "\n",
        "app=graph.compile()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bMomibQbEYPh"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output=app.invoke({'input_query':'My name is Sam@gmail.com and I need data for Sales data for l/12/2024'})"
      ],
      "metadata": {
        "id": "GCNtYR-hNJLr"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eBR6GxV2OFfL"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U5wlQSkOhCV",
        "outputId": "72ea1b1a-ef45-4a5f-8cda-c81d24ddd776"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user_name': 'Sam@gmail.com',\n",
              " 'topic': 'Sales data',\n",
              " 'input_query': 'My name is Sam@gmail.com and I need data for Sales data for l/12/2024',\n",
              " 'due_date': '01/12/2024',\n",
              " 'validation': 'yes',\n",
              " 'messages': 'Your necessary informations are stored         Username        Date       Topic\\n0  Sam@gmail.com  01/12/2024  Sales data. Mail has sent to your mailbox '}"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Human Looped Chatbot"
      ],
      "metadata": {
        "id": "9mYn-u2RchNu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-WvBHdwe6pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2V1cojsyeFOD"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "VuPygVDuea95"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HM8IqE3rhQuy"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ra_agent():\n",
        "  def format_docs(docs):\n",
        "        return \"\\n\\n\".join(content.page_content for content in docs )\n",
        "  db_persist_directory = \"/content/drive/MyDrive/chroma_db_let_us_C\"\n",
        "  drive_base_path='/content/drive/MyDrive'\n",
        "  hf_models_folder = 'HuggingFace_Models'\n",
        "  model_name_to_save = 'all-MiniLM-L6-v2'\n",
        "  hf_model_load_path = os.path.join(drive_base_path, hf_models_folder, model_name_to_save)\n",
        "\n",
        "  embedding_instance=HuggingFaceEmbeddings(model_name=model_name_to_save)\n",
        "  chroma_db=Chroma(embedding_function=embedding_instance,persist_directory=db_persist_directory)\n",
        "  retriever=chroma_db.as_retriever(kwargs={'k':3})\n",
        "  llm=ChatGroq(model='llama-3.1-8b-instant')\n",
        "  multi_retriever=MultiQueryRetriever.from_llm(retriever=retriever,llm=llm)\n",
        "\n",
        "  return multi_retriever\n",
        "class Agent(TypedDict):\n",
        "  question:str\n",
        "  query_type:str\n",
        "  response:Annotated[List[HumanMessage],add_messages]\n",
        "\n",
        "@tool\n",
        "def retrieve_information(question) ->str:\n",
        "    \"Retrieve necessary docs from vectorstore for user query and reframe it to give proper answer\"\n",
        "\n",
        "    multi_retriever=get_ra_agent()\n",
        "\n",
        "    output=multi_retriever.invoke(question)\n",
        "\n",
        "    print(f'No of documents retrieved is {len(output)}')\n",
        "\n",
        "    return'\\n\\n'.join([doc.page_content for doc in output])\n",
        "\n",
        "def retriever(agent:Agent):\n",
        "    context=retrieve_information.invoke({'question':agent['question']})\n",
        "    prompt=PromptTemplate(input_varibles=['question','context'],\n",
        "                          template='You are an RAG based assistant who will be helpful in answering user query{query} with hel pf retrieved context{context}')\n",
        "    llm=ChatGroq(model='llama-3.1-8b-instant')\n",
        "    answer=llm.invoke(prompt.format(query=agent['question'],context=context))\n",
        "    return {'response':answer}\n",
        "\n",
        "def router(agent:Agent):\n",
        "\n",
        "    llm=ChatGroq(model='llama-3.1-8b-instant')\n",
        "\n",
        "    query_type=llm.invoke(f\"\"\"Classify whether user query is question or answer .\n",
        "                            user_query - {agent['question']}\n",
        "                            Kindly give output only in one word 'question' or 'end'   \"\"\").content\n",
        "    if query_type.lower()=='question' or 'question' in query_type.lower():\n",
        "      return {'query_type':'question'}\n",
        "    else:\n",
        "      return{'query_type':'end'}\n",
        "\n",
        "\n",
        "\n",
        "graph=StateGraph(Agent)\n",
        "\n",
        "graph.add_node('retriever',retriever)\n",
        "graph.add_node('router',router)\n",
        "\n",
        "graph.set_entry_point('router')\n",
        "graph.add_conditional_edges('router',\n",
        "                            lambda agent : 'question' if agent['query_type'] not in ['farewell','end','exit'] else 'end',\n",
        "                            {'question':'retriever',\n",
        "                             'end':END})\n",
        "graph.add_edge('retriever',END)\n",
        "\n",
        "app=graph.compile()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7mgBZLwQcgPC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vR_s4c09yoJE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer=app.invoke({'question':'What is functions'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYhgpPX9ko1U",
        "outputId": "053a109f-b150-43ad-8fc5-3f3330daae3c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of documents retrieved is 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in answer['response']:\n",
        "  m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6P7rM5zl1Jx",
        "outputId": "4b9274a4-7ddf-4789-92b7-5434284a4589"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Based on the provided text, I will provide an explanation of functions with the help of the retrieved context.\n",
            "\n",
            "**What is a Function?**\n",
            "\n",
            "A function is a self-contained block of statements that perform a coherent task of some kind. It is a way to isolate code that is related to a specific task, making it easier to reuse and maintain. Every C program can be thought of as a collection of these functions.\n",
            "\n",
            "**Characteristics of Functions**\n",
            "\n",
            "1. **Self-contained**: A function is a block of code that performs a specific task and can be used independently.\n",
            "2. **Coherent task**: A function performs a single, well-defined task.\n",
            "3. **Reusable**: Functions can be reused throughout a program.\n",
            "4. **Maintainable**: Functions are easier to maintain and modify than a large block of code.\n",
            "\n",
            "**Declaring and Defining Functions**\n",
            "\n",
            "1. **Function declaration**: Specifies the return type of the function and the types of parameters it accepts.\n",
            "2. **Function definition**: Defines the body of the function, where the code that performs the task is written.\n",
            "\n",
            "**Variables in Functions**\n",
            "\n",
            "1. **Scope**: Variables declared in a function are only available within that function and are not accessible outside of it.\n",
            "\n",
            "**Types of Functions**\n",
            "\n",
            "1. **API (Application Programming Interface) functions**: These are functions that are used to interact with other programs or libraries.\n",
            "\n",
            "**Example Use Case**\n",
            "\n",
            "In the provided text, the example of a library with a structure to hold book information is used to illustrate the concept of functions. The functions include:\n",
            "\n",
            "1. `add_book_info()`: Adds book information to the library.\n",
            "2. `display_book_info()`: Displays book information.\n",
            "3. `list_books_by_author()`: Lists all books by a given author.\n",
            "4. `list_book_title()`: Lists the title of a specified book.\n",
            "5. `count_books()`: Lists the count of books in the library.\n",
            "6. `list_books_by_accession_number()`: Lists books in the order of accession number.\n",
            "7. `exit()`: Exits the program.\n",
            "\n",
            "These functions demonstrate the concept of self-contained blocks of code that perform a specific task and can be reused throughout the program.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ag=get_ra_agent()"
      ],
      "metadata": {
        "id": "LVWWrpJc6cR9"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ag.invoke('What is pointers')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8ZnnTxk__or",
        "outputId": "8b5b5a0d-9104-40f7-9a01-31280e91379b"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vLXx2buDADSe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}