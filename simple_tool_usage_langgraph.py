# -*- coding: utf-8 -*-
"""Simple tool usage -Langgraph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WwrQitod2SycTsX1Xcp558IFZCVKTkmB
"""

!pip install langgraph langchain_core langchain_groq langchain_huggingface langchain_chroma



from typing import Annotated,List,Literal
from typing_extensions import TypedDict
from langgraph.graph import START,END,StateGraph
from langgraph.prebuilt import ToolNode,tools_condition
from langgraph.graph.message  import add_messages
from langchain.chat_models import init_chat_model
from langchain_huggingface import HuggingFaceEmbeddings
from google.colab import userdata,drive
from langgraph.graph.message import add_messages
from langchain_chroma import Chroma
from langchain_core.tools import tool
drive.mount('/content/drive')
import os
os.environ['GROQ_API_KEY']=userdata.get('GROQ_API_KEY')
db_persist_directory = "/content/drive/MyDrive/chroma_db_let_us_C"
drive_base_path='/content/drive/MyDrive'
hf_models_folder = 'HuggingFace_Models'
model_name_to_save = 'all-MiniLM-L6-v2'
hf_model_load_path = os.path.join(drive_base_path, hf_models_folder, model_name_to_save)
embedding_instance=HuggingFaceEmbeddings(model_name=model_name_to_save)

@tool
def perform_math_operations(a:float , b:float, maths_operation:str) ->float:
  "Use this for functions for maths operations such as add, sub, multiply, divide and ensure you are foloowing BODMAS rule"
  if 'add' in maths_operation:
      return a+b
  elif 'sub' in maths_operation:
    return a-b
  elif 'multiply' in maths_operation:
    return a*b
  elif 'divide' in maths_operation:
    return a//b
  else:
    return None

#llm call
llm= init_chat_model(model='gemma2-9b-it',model_provider='groq')

avail_tools=[perform_math_operations]

llm_with_tool=llm.bind_tools(avail_tools)

#retreiver
vector_db=Chroma(embedding_function=embedding_instance,persist_directory=db_persist_directory).as_retriever(kwargs={'k':5})





class State(TypedDict):
  messages: Annotated[List,add_messages]

def llm_call(state:State):
  return {'messages':llm_with_tool.invoke(state['messages'])}

graph=StateGraph(State)
graph.add_node('tool_llm',llm_call)
graph.add_node('tools',ToolNode(avail_tools))

graph.add_edge(START,'tool_llm')
graph.add_conditional_edges('tool_llm',tools_condition)
graph.add_edge('tools','tool_llm')

builder=graph.compile()

message=builder.invoke({'messages':'find solution for this 5 + 2 + 4 - 10* 6? Ensure you are using BODMAS rule'})

for m in message['messages']:
  m.pretty_print()

async for events in builder.astream_events({'messages':'What is add of 5 and 2? '},stream_mode='values'):
  print(events)



async for events in builder.astream_events({'messages':'What is add of 5 and 2 and 18? '},stream_mode='values'):

    print(events)

